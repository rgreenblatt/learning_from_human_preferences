# learning_from_human_preferences

Requires [this branch](https://github.com/neevparikh/pfrl/tree/master) of pfrl

## TODO

- [ ] Actual ensemble for reward model and uncertainty based trajectory selection
- [ ] Implement UI for human prefs selection
- [ ] Implement async human prefs input
- [ ] Run on a bunch of envs and see what is/isn't working
- [ ] Consider tuning some params...

## Long TODO
- [ ] Run/test/debug actually using human prefs
- [ ] Swap to language model task
